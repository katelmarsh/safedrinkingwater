
---
title: "Water Quality"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data from NOAA about storm events from 1951-today 
frustrating that they do not have an API, why is it a CSV download of like 35 files 
Link: https://www.ncdc.noaa.gov/stormevents/listevents.jsp?eventType=%28Z%29+Winter+Storm&beginDate_mm=01&beginDate_dd=01&beginDate_yyyy=2000&endDate_mm=12&endDate_dd=31&endDate_yyyy=2021&county=ALL&hailfilter=0.00&tornfilter=0&windfilter=000&sort=DT&submitbutton=Search&statefips=48%2CTEXAS 

this is preliminary work before I combined and pared down the bulk data. 
```{r}
hurricanes <- read.csv("~/Downloads/storm_data_tx_hurricane.csv")
trop_storm <- read.csv("~/Downloads/storm_data_tropstrom_tropdepres.csv")
coastal <- read.csv("~/Downloads/storm_data_coastal_flood.csv")

colnames(hurricanes) == colnames(trop_storm)

extreme_weather <- rbind(hurricanes, trop_storm)

lapply(extreme_weather, class)
summary(extreme_weather$INJURIES_INDIRECT)
extreme_weather[which(extreme_weather$INJURIES_INDIRECT == 2400),] # max is hurricane Ike (which I was in Houston for)

```


## NOAA data part 2 
now I'm trying to just use the bulk data then pare down from there because the website is so annoying and they only give 500 searched results at a time or bulk downloads 
```{r}
library(data.table)
library(dplyr)
library(tidyr)
library(purrr)

dt = fread("~/Downloads/StormEvents_details-ftp_v1.0_d1950_c20170120.csv.gz")

files <- dir("~/Downloads/storm_events", pattern = "*.csv.gz")

data <- paste0("~/Downloads/storm_events/", files) %>%
  map(fread)
unique(lapply(data, length)) # length of 51 and 11 


#made function to find length of a element in a list
lengthIs <- function(n) function(x) length(x)==n

# storm details
data_51 <- do.call(rbind, Filter(lengthIs(51), data))

data_fat <-  data[72:139] # manually removed the years / csvs with no data 
# fatality data 
data_fat <- do.call(rbind, Filter(lengthIs(11), data_fat))

data_loc <-  data[188:214]                 
# location data 
data_loc <- rbindlist(data_loc, fill=TRUE)

data_51 <- data_51[data_51$STATE == "TEXAS"] # weather details for just texas 
most_dat <- merge(data_51, data_fat, by = "EVENT_ID", all.x = TRUE) # weather and fatalities for texas 
tx_weather <- merge(most_dat, data_loc, by = "EVENT_ID", all.x = TRUE) # final dataset

write.csv(tx_weather, "~/Downloads/tx_weather_noaa.csv")

```

#Exploring the TX Weather Data 

```{r}
tx_weather <- fread("~/Downloads/tx_weather_noaa.csv")

#Paulina's code for filtering and cleaning property damage $ 
tx_weather=tx_weather %>% filter(EVENT_TYPE%in%events) %>% mutate(Z_FIPS=case_when(
  CZ_FIPS<100&CZ_FIPS>9~paste0(0,CZ_FIPS),
  CZ_FIPS<10~paste0("00",CZ_FIPS),
  CZ_FIPS>99~paste0(CZ_FIPS))) %>% 
  mutate(Z_FIPS=paste0(STATE_FIPS,Z_FIPS), DAMAGE_PROPERTY_NUM= as.numeric(gsub("[a-zA-Z ]", "", DAMAGE_PROPERTY)),
         Q=str_sub(DAMAGE_PROPERTY,-1,-1)) %>% 
  mutate(DAMAGE_PROPERTY_NUM=case_when( #Getting numeric values for property damage
    Q=="K"~DAMAGE_PROPERTY_NUM*1000,
    Q=="M"~DAMAGE_PROPERTY_NUM*1000000,
    Q=="B"~DAMAGE_PROPERTY_NUM*1000000000,
    Q==0~DAMAGE_PROPERTY_NUM
    ))
#The Zone FIPS do not correspond to county FIPS. Texas has only 254 counties 

classes <- lapply(tx_weather, class)
numeric <- which(classes == "numeric")

out <- lm(MAGNITUDE ~ BEGIN_DAY + BEGIN_TIME + END_DAY + END_TIME + YEAR + BEGIN_LAT + BEGIN_LON + END_LAT + END_LON + INJURIES_DIRECT + DEATHS_DIRECT + DEATHS_INDIRECT + INJURIES_INDIRECT, data=tx_weather)
summary(out)
plot(out)

# checking to see the counties listed for the weather events -- there are 276 unique 
length(unique(tx_weather$CZ_NAME[which(tx_weather$CZ_TYPE == 'C')]))

hist(tx_weather$YEAR)

plot(tx_weather$YEAR, tx_weather$DAMAGE_PROPERTY_NUM)

counts <- table(tx_weather$MONTH_NAME)
barplot(counts, main="Event Month",las=2, cex.names=.75)

counts <- table(tx_weather$BEGIN_TIME)
barplot(counts, main="Event Beginning Time",las=2, cex.names=.75)

counts <- table(tx_weather$EVENT_TYPE)
barplot(counts, main="Event Type",las=2, cex.names=.5)

```
Interesting regression here. When using magnitude as the indicator variable, we find a significant relationship between it and beginning lat/long, ending lat/long, and year. We don't find any relationship between that and injuries/deaths or beginning day/time or ending day/time. However, it is probably a bad indicator variable because there at 57,000 NA variables in the magnitude column. Magnitude refers to only wind speed and hail size. 

## SDWIS Federal Reports on Texas PWS 
Link: https://ofmpub.epa.gov/apex/sfdw/f?p=108:21::::RP,RIR::

```{r}
sdwis <- read.csv("~/Downloads/water_system_summary_SDWIS_FED_Reports.csv")

lapply(sdwis, class)

# cleaning! removing commas from population count and making numeric
y <- c(sdwis$Population.Served.Count)
sdwis$Population.Served.Count = as.numeric(gsub(",", "", y))
summary(sdwis$Population.Served.Count)

# violations 
hist(sdwis$X..of.Violations)
summary(sdwis$X..of.Violations) 
# one of them has 2411?! but it looks like most are under 10 
sort(table(sdwis$Counties.Served[which(sdwis$X..of.Violations > 22)])) # seeing if the top quartile of violations are concentrated in certain counties -- it appears that Harris county (Houston), Brazoria, Montgomery, and Lubbock are the top offenders of counties with lots of water systems in the top quartile -- this should probably be indexed for amount of PWS in each county, but it is interesting that Harris county has such a larger amt than Dallas 

plot(sdwis$X..of.Violations, sdwis$Population.Served.Count, main = "Violations vs Population Served")


```


```{r}
reg <- lm(X..of.Violations ~ Population.Served.Count + X..of.Site.Visits + X..of.Facilities, data=sdwis)
summary(reg)
# find significant relationship between violation count and site visit count
plot(sdwis$X..of.Site.Visits, sdwis$X..of.Violations)


reg <- lm(Population.Served.Count ~  X..of.Violations + X..of.Site.Visits + X..of.Facilities, data=sdwis)
summary(reg)
# find a significant relationship between population served and site visits or facilities 
plot(sdwis$Population.Served.Count, sdwis$X..of.Facilities, main = "population served vs number of facilities")
plot(sdwis$X..of.Site.Visits, sdwis$Population.Served.Count, main = "number of site visits vs population served")
reg <- lm(X..of.Site.Visits ~  X..of.Violations + Population.Served.Count + X..of.Facilities, data=sdwis)
summary(reg)
plot(reg)
```
We find significant relationship between violation count and site visit count. We also find a a significant relationship between population served and site visits or facilities. These relationships follow common sense. PWSs with large amounts of violations probably have more visits. Similarly, PWSs that serve a larger population probably have more facilties and site visits. 

```{r}
weather_counties <- tx_weather$CZ_NAME[which(tx_weather$CZ_TYPE == 'C')] # names of counties in the tx_weather set
length(unique(weather_counties)) # checking for how many county names there are. there are 276 whcih is much above the amt of 254. since I need cleaning for both this dataset and the other one, I'm going to get a set of county names to double check. 
county_list <- read.csv("~/Downloads/tx_counties.csv") # double checking CSV

'%!in%' <- function(x,y)!('%in%'(x,y)) # lil function to make it easier

wrong_counties <- unique(weather_counties[toupper(weather_counties) %!in% toupper(county_list$County.Name)]) # list of counties that are not in the standard list, need cleaning

## SDWIS Counties 
# Counties
unique(sdwis$EPA.Region) # they're all in region 6 which is Texas 

length(unique(sdwis$Counties.Served)) # only 256, 2 extra counties 
wrong_counties_2 <-  unique(sdwis$Counties.Served[toupper(sdwis$Counties.Served) %!in% toupper(county_list$County.Name)])  # list of counties that are not in the standard list, need cleaning 
# after looking at this looks like "Dallas, Johnson" and "Coke, Concho, Ector, Howard, Martin, Scurry, Ward" are the extra counties 

```
Trying to figure out how to combine given that multiple PWSs service one county and different parts of one county. 
https://www.dshs.texas.gov/chs/info/info_txco.shtm -- decided to grab a CSV of official county names and FIPS codes from the Texas Government, this one is from the Department of State Health Services. 


```{r}
library(ggplot2)

filt_weather <- tx_weather[tx_weather$EVENT_TYPE == c("Flash Flood","Hurricane","Coastal Flood", "Flood", "Storm Surge/Tide", "Heavy Rain", "Hurricane (Typhoon)", "Tropical Storm", "Tropical Depression", "THUNDERSTORM WINDS/ FLOOD", "Drought")]


qplot(BEGIN_LON, BEGIN_LAT, data=filt_weather, colour=EVENT_TYPE, 
      main = "Map of Event Beginning Locations in Texas")
qplot(END_LON, END_LAT, data=filt_weather, colour=EVENT_TYPE, 
      main = "Map of Event Ending Locations in Texas")

#+ 
#      borders("county") + scale_size_area()

table(filt_weather$EVENT_TYPE[which(is.na(filt_weather$BEGIN_LAT | filt_weather$BEGIN_LON))])
# missing NA values for mostly flash floods and floods

table(filt_weather$EVENT_TYPE[which(is.na(filt_weather$LATITUDE | filt_weather$LONGITUDE))])
# missing more for flash floods and floods, mostly the same missing location data 

table(filt_weather$EVENT_TYPE[which(is.na(filt_weather$BEGIN_LAT | filt_weather$BEGIN_LON))])

```
##Spatial Join (I deleted all of my own code, this is Paulina's)

```{r}
library(tidyr)
library(rgdal)

County_Public_Forecast_Zones_Correlation_file <- read_csv("~/Downloads/County-Public Forecast Zones Correlation file.csv")
TX_FIPS=County_Public_Forecast_Zones_Correlation_file %>% filter(STATE=="TX")
TX_sh=shapefile("~/Downloads/geo_export_2fa04bf9-5c8d-4a39-8dda-98d943ae3202.shp")


tx_weather_fil=tx_weather %>% drop_na(c(LONGITUDE, LATITUDE))
coordinates(tx_weather_fil) <- c("LONGITUDE","LATITUDE")
as(tx_weather_fil,"SpatialPoints")
proj4string(tx_weather_fil) <- CRS("+proj=longlat +datum=WGS84")
proj4string(TX_sh) <- CRS("+proj=longlat +datum=WGS84")

mer=over(tx_weather_fil,TX_sh)
data=cbind(mer,tx_weather_fil@data)
data=data %>% dplyr::select(-date_date_, -time_date_,-date_dat_2,-time_dat_2,-date_dat_3,
                     -time_dat_3,-shape_area,-source,-stratmap_i) %>% mutate(FIPS=paste0(STATE_FIPS,fips_code))

```

## Double Checking 


