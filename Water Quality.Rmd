
---
title: "Water Quality"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data from NOAA about storm events from 1951-today 
frustrating that they do not have an API, why is it a CSV download of like 35 files 
Link: https://www.ncdc.noaa.gov/stormevents/listevents.jsp?eventType=%28Z%29+Winter+Storm&beginDate_mm=01&beginDate_dd=01&beginDate_yyyy=2000&endDate_mm=12&endDate_dd=31&endDate_yyyy=2021&county=ALL&hailfilter=0.00&tornfilter=0&windfilter=000&sort=DT&submitbutton=Search&statefips=48%2CTEXAS 

this is preliminary work before I combined and pared down the bulk data. 
```{r}
hurricanes <- read.csv("~/Downloads/storm_data_tx_hurricane.csv")
trop_storm <- read.csv("~/Downloads/storm_data_tropstrom_tropdepres.csv")
coastal <- read.csv("~/Downloads/storm_data_coastal_flood.csv")

colnames(hurricanes) == colnames(trop_storm)

extreme_weather <- rbind(hurricanes, trop_storm)

lapply(extreme_weather, class)
summary(extreme_weather$INJURIES_INDIRECT)
extreme_weather[which(extreme_weather$INJURIES_INDIRECT == 2400),] # max is hurricane Ike (which I was in Houston for)

```


## NOAA data part 2 
now I'm trying to just use the bulk data then pare down from there because the website is so annoying and they only give 500 searched results at a time or bulk downloads 
```{r}
library(data.table)
library(dplyr)
library(tidyr)
library(purrr)

dt = fread("~/Downloads/StormEvents_details-ftp_v1.0_d1950_c20170120.csv.gz")

files <- dir("~/Downloads/storm_events", pattern = "*.csv.gz")

data <- paste0("~/Downloads/storm_events/", files) %>%
  map(fread)
unique(lapply(data, length)) # length of 51 and 11 


#made function to find length of a element in a list
lengthIs <- function(n) function(x) length(x)==n

# storm details
data_51 <- do.call(rbind, Filter(lengthIs(51), data))

data_fat <-  data[72:139] # manually removed the years / csvs with no data 
# fatality data 
data_fat <- do.call(rbind, Filter(lengthIs(11), data_fat))

data_loc <-  data[188:214]                 
# location data 
data_loc <- rbindlist(data_loc, fill=TRUE)

data_51 <- data_51[data_51$STATE == "TEXAS"] # weather details for just texas 
most_dat <- merge(data_51, data_fat, by = "EVENT_ID", all.x = TRUE) # weather and fatalities for texas 
tx_weather <- merge(most_dat, data_loc, by = "EVENT_ID", all.x = TRUE) # final dataset

write.csv(tx_weather, "~/Downloads/tx_weather_noaa.csv")

```

#Exploring the TX Weather Data 

```{r}
library(data.table)
library(stringr)
tx_weather <- fread("~/Downloads/tx_weather_noaa_ver_3.csv")

#Paulina's code for filtering and cleaning property damage $ 
events=c("Flash Flood","Drought", "Flood","Heavy Rain","Hurricane (Typhoon)",
         "Tropical Storm","Tropical Depression","Hurricane","THUNDERSTORM WINDS/ FLOOD")

tx_weather=tx_weather %>% filter(EVENT_TYPE%in%events) %>% mutate(Z_FIPS=case_when(
  CZ_FIPS<100&CZ_FIPS>9~paste0(0,CZ_FIPS),
  CZ_FIPS<10~paste0("00",CZ_FIPS),
  CZ_FIPS>99~paste0(CZ_FIPS))) %>% 
  mutate(Z_FIPS=paste0(STATE_FIPS,Z_FIPS), DAMAGE_PROPERTY_NUM= as.numeric(gsub("[a-zA-Z ]", "", DAMAGE_PROPERTY)),
         Q=str_sub(DAMAGE_PROPERTY,-1,-1)) %>% 
  mutate(DAMAGE_PROPERTY_NUM=case_when( #Getting numeric values for property damage
    Q=="K"~DAMAGE_PROPERTY_NUM*1000,
    Q=="M"~DAMAGE_PROPERTY_NUM*1000000,
    Q=="B"~DAMAGE_PROPERTY_NUM*1000000000,
    Q==0~DAMAGE_PROPERTY_NUM
    ))
#The Zone FIPS do not correspond to county FIPS. Texas has only 254 counties 

classes <- lapply(tx_weather, class)
numeric <- which(classes == "numeric")

out <- lm(DAMAGE_PROPERTY_NUM ~ BEGIN_DAY + BEGIN_TIME + END_DAY + END_TIME + YEAR + BEGIN_LAT + BEGIN_LON + END_LAT + END_LON + INJURIES_DIRECT + DEATHS_DIRECT + DEATHS_INDIRECT + INJURIES_INDIRECT, data=tx_weather)
summary(out)
plot(out)

# checking to see the counties listed for the weather events -- there are 276 unique 
length(unique(tx_weather$CZ_NAME[which(tx_weather$CZ_TYPE == 'C')]))

hist(tx_weather$YEAR)

plot(tx_weather$YEAR, tx_weather$DAMAGE_PROPERTY_NUM)

counts <- table(tx_weather$MONTH_NAME)
barplot(counts, main="Event Month",las=2, cex.names=.75)

counts <- table(tx_weather$BEGIN_TIME)
barplot(counts, main="Event Beginning Time",las=2, cex.names=.75)

counts <- table(tx_weather$EVENT_TYPE)
barplot(counts, main="Event Type",las=2, cex.names=.5)

```
Interesting regression here. When using magnitude as the indicator variable, we find a significant relationship between it and beginning lat/long, ending lat/long, and year. We don't find any relationship between that and injuries/deaths or beginning day/time or ending day/time. However, it is probably a bad indicator variable because there at 57,000 NA variables in the magnitude column. Magnitude refers to only wind speed and hail size. 


```{r}
library(dplyr)
library(purrr)
library(data.table)

hist(data$YEAR)
  
files <- dir("~/Desktop/storm_events", pattern = "*.csv.gz")

data_gz <- paste0("~/Desktop/storm_events/", files) %>%
  map(fread)
unique(lapply(data_gz, length)) # length of 51 and 11 

#made function to find length of a element in a list
lengthIs <- function(n) function(x) length(x)==n

# storm details
data_51 <- do.call(rbind, Filter(lengthIs(51), data_gz))

data_fat <- Filter(lengthIs(11), data_gz)
data_fatalities <- do.call(rbind, data_fat[1:4])
data_fatalities_2 <- do.call(rbind, data_fat[5:8])

data_51 <- data_51[data_51$STATE == "TEXAS"] # weather details for just texas 

most_dat <- merge(data_51, data_fatalities, by = "EVENT_ID", all.x = TRUE) # weather and fatalities for texas 
tx_weather_2 <- merge(most_dat, data_fatalities_2, by = "EVENT_ID", all.x = TRUE) # final dataset

tx_weather <- read.csv("~/Downloads/tx_weather_noaa.csv")

tx_weather$X = NULL

tx_weather_full <- rbind(tx_weather,tx_weather_2)

write.csv(tx_weather_full, "~/Downloads/tx_weather_noaa_ver_3.csv")

```

## SDWIS Federal Reports on Texas PWS 
Link: https://ofmpub.epa.gov/apex/sfdw/f?p=108:21::::RP,RIR::

```{r}
sdwis <- read.csv("~/Downloads/water_system_summary_SDWIS_FED_Reports.csv")

lapply(sdwis, class)

# cleaning! removing commas from population count and making numeric
y <- c(sdwis$Population.Served.Count)
sdwis$Population.Served.Count = as.numeric(gsub(",", "", y))
summary(sdwis$Population.Served.Count)

# violations 
hist(sdwis$X..of.Violations)
summary(sdwis$X..of.Violations) 
# one of them has 2411?! but it looks like most are under 10 
sort(table(sdwis$Counties.Served[which(sdwis$X..of.Violations > 22)])) # seeing if the top quartile of violations are concentrated in certain counties -- it appears that Harris county (Houston), Brazoria, Montgomery, and Lubbock are the top offenders of counties with lots of water systems in the top quartile -- this should probably be indexed for amount of PWS in each county, but it is interesting that Harris county has such a larger amt than Dallas 

plot(sdwis$X..of.Violations, sdwis$Population.Served.Count, main = "Violations vs Population Served")


```


```{r}
reg <- lm(X..of.Violations ~ Population.Served.Count + X..of.Site.Visits + X..of.Facilities, data=sdwis)
summary(reg)
# find significant relationship between violation count and site visit count
plot(sdwis$X..of.Site.Visits, sdwis$X..of.Violations)


reg <- lm(Population.Served.Count ~  X..of.Violations + X..of.Site.Visits + X..of.Facilities, data=sdwis)
summary(reg)
# find a significant relationship between population served and site visits or facilities 
plot(sdwis$Population.Served.Count, sdwis$X..of.Facilities, main = "population served vs number of facilities")
plot(sdwis$X..of.Site.Visits, sdwis$Population.Served.Count, main = "number of site visits vs population served")
reg <- lm(X..of.Site.Visits ~  X..of.Violations + Population.Served.Count + X..of.Facilities, data=sdwis)
summary(reg)
plot(reg)
```
We find significant relationship between violation count and site visit count. We also find a a significant relationship between population served and site visits or facilities. These relationships follow common sense. PWSs with large amounts of violations probably have more visits. Similarly, PWSs that serve a larger population probably have more facilties and site visits. 

```{r}
weather_counties <- tx_weather$CZ_NAME[which(tx_weather$CZ_TYPE == 'C')] # names of counties in the tx_weather set
length(unique(weather_counties)) # checking for how many county names there are. there are 276 whcih is much above the amt of 254. since I need cleaning for both this dataset and the other one, I'm going to get a set of county names to double check. 
county_list <- read.csv("~/Downloads/tx_counties.csv") # double checking CSV

'%!in%' <- function(x,y)!('%in%'(x,y)) # lil function to make it easier

wrong_counties <- unique(weather_counties[toupper(weather_counties) %!in% toupper(county_list$County.Name)]) # list of counties that are not in the standard list, need cleaning

## SDWIS Counties 
# Counties
unique(sdwis$EPA.Region) # they're all in region 6 which is Texas 

length(unique(sdwis$Counties.Served)) # only 256, 2 extra counties 
wrong_counties_2 <-  unique(sdwis$Counties.Served[toupper(sdwis$Counties.Served) %!in% toupper(county_list$County.Name)])  # list of counties that are not in the standard list, need cleaning 
# after looking at this looks like "Dallas, Johnson" and "Coke, Concho, Ector, Howard, Martin, Scurry, Ward" are the extra counties 

```
Trying to figure out how to combine given that multiple PWSs service one county and different parts of one county. 
https://www.dshs.texas.gov/chs/info/info_txco.shtm -- decided to grab a CSV of official county names and FIPS codes from the Texas Government, this one is from the Department of State Health Services. 


```{r}
library(ggplot2)

filt_weather <- tx_weather[tx_weather$EVENT_TYPE == c("Flash Flood","Hurricane","Coastal Flood", "Flood", "Storm Surge/Tide", "Heavy Rain", "Hurricane (Typhoon)", "Tropical Storm", "Tropical Depression", "THUNDERSTORM WINDS/ FLOOD", "Drought")]


qplot(BEGIN_LON, BEGIN_LAT, data=filt_weather, colour=EVENT_TYPE, 
      main = "Map of Event Beginning Locations in Texas")
qplot(END_LON, END_LAT, data=filt_weather, colour=EVENT_TYPE, 
      main = "Map of Event Ending Locations in Texas")

#+ 
#      borders("county") + scale_size_area()

table(filt_weather$EVENT_TYPE[which(is.na(filt_weather$BEGIN_LAT | filt_weather$BEGIN_LON))])
# missing NA values for mostly flash floods and floods

table(filt_weather$EVENT_TYPE[which(is.na(filt_weather$LATITUDE | filt_weather$LONGITUDE))])
# missing more for flash floods and floods, mostly the same missing location data 

table(filt_weather$EVENT_TYPE[which(is.na(filt_weather$BEGIN_LAT | filt_weather$BEGIN_LON))])

```
##Spatial Join (I deleted all of my own code, this is Paulina's)

```{r}
library(tidyr)
library(rgdal)
library(readr)
library(raster)
library(dplyr)

County_Public_Forecast_Zones_Correlation_file <- read_csv("~/Downloads/County-Public Forecast Zones Correlation file.csv")
TX_FIPS=County_Public_Forecast_Zones_Correlation_file %>% filter(STATE=="TX")
TX_sh=shapefile("~/Downloads/geo_export_2fa04bf9-5c8d-4a39-8dda-98d943ae3202.shp")


tx_weather_fil=tx_weather %>% drop_na(c(LONGITUDE, LATITUDE))
coordinates(tx_weather_fil) <- c("LONGITUDE","LATITUDE")
as(tx_weather_fil,"SpatialPoints")
proj4string(tx_weather_fil) <- CRS("+proj=longlat +datum=WGS84")
proj4string(TX_sh) <- CRS("+proj=longlat +datum=WGS84")

mer=over(tx_weather_fil,TX_sh)
data=cbind(mer,tx_weather_fil@data)
data=data %>% dplyr::select(-date_date_, -time_date_,-date_dat_2,-time_dat_2,-date_dat_3,
                     -time_dat_3,-shape_area,-source,-stratmap_i) %>% mutate(FIPS=paste0(STATE_FIPS,fips_code))

```

## Double Checking 
Cleaning CZ_FIPS in 'data' 
```{r}
library(NLP)
head(data)
data$CZ_FIPS <- sprintf("%03d", as.numeric(data$CZ_FIPS))
data$CZ_FIPS <- as.numeric(data$CZ_FIPS) + 48000
length(unique(data$CZ_FIPS)) #258??? 

compared_fips <- data[which(data$fips_code != data$CZ_FIPS),]
# from the "episode narrative" it is pretty clear that the Long/Lat do not necessarily line up with the counties for some of the reported. Most of the time it is the same storm listed multiple times with different long/lats 
dim(compared_fips) # 1407 of the things were changed from original CZ_fips 
table(compared_fips$CZ_TYPE) 
# most of the changed fips codes were C rather than Z, looks like most of the "z" codes did not line up because they were actually the correct C codes 
table(compared_fips$CZ_TYPE)["Z"]/table(compared_fips$CZ_TYPE)["C"] 
# most of the changed fips codes were C rather than Z, looks like most of the "z" codes did not line up because they were actually the correct C codes 
table(data$CZ_TYPE)["Z"]/table(data$CZ_TYPE)["C"] 
# this amt makes sense compared to the amount of C and Z types in the original "data" dataset, although Z is .002 of the original data and .01 of the compared_fips. 

data[which(data$CZ_FIPS %!in% TX_FIPS$FIPS),] # what to do with mulitple listed counties? 
TX_FIPS$FIPS["48050"] # this FIPS code does not exist, the fips code for Andrews county is 48003
TX_FIPS$FIPS["48074"] # this FIPS code does not exist, the fips code for Jeff Davis county is 48243, I believe they are referring to the Fort Davis Park or the Davis Mountains, which are both located in Jeff Davis County. 
TX_FIPS$FIPS["48144"] # this FIPS code does not exist, the fips code for Bosque county is 48035
TX_FIPS$FIPS["48156"] # this FIPS code does not exist, the fips code for Lampasas county is 48281
TX_FIPS$FIPS["48134"] # this FIPS code does not exist, the fips code for Ellis county is 48139
TX_FIPS$FIPS["48142"] # this FIPS code does not exist, the fips code for Mills county is 48333

FIPS <- data$CZ_FIPS
new_listed <- rep(NA, length(FIPS))
for(i in 1:length(FIPS)) {
  my_output <- switch(as.String(FIPS[i]), "48050" = "48003", "48074" = "48243", 
                      "48144" = "48035", "48156"= "48281", "48134" = "48139", "48142"="48333")
  if(is.null(my_output)){
    new_listed[i] <- FIPS[i]
  }
  else {
    new_listed[i] <- my_output
  }
}
TX_FIPS$FIPS[which(TX_FIPS$FIPS %!in% new_listed)] # 48417 is not in the new_listed column 
# Seems like these counties are not in the original CZ_FIPS file and do not have any extreme weather events 
length(new_listed) == length(FIPS) # same length, same amount of elements 
data$CZ_FIPS = as.numeric(new_listed)

head(data$CZ_FIPS)
head(TX_FIPS$FIPS)

```


```{r}
# the sdwis I have doesn't have the fips, neither does the website 
head(TX_FIPS)
sdwis_fips <- merge(TX_FIPS, sdwis, by.x="COUNTY", by.y="Counties.Served", all.y=TRUE)
length(unique(sdwis_fips$FIPS)) # 255??? 

# Comparing against TX_FIPS file 
length(unique(TX_FIPS$FIPS)) # 254, should be an accurate account of FIPS and county amounts 

sdwis_fips[which(sdwis_fips$FIPS %!in% TX_FIPS$FIPS),] # what to do with mulitple listed counties? 
# Row 1823 and 2159 do not have a corresponding FIPS code 
# Finding primary county online.... 

sdwis_fips[-c(1823, 2159),]

sdwis_fips[which(TX_FIPS$FIPS %!in% sdwis_fips$FIPS),] # thw SDWIS file includes all of the FIPS codes in the TX_FIPS file 

TX_FIPS[which(TX_FIPS$COUNTY %in% c("Coke", "Concho", "Ector", "Howard", "Martin", "Scurry", "Ward")),]
# this is a municipal water district - should be serving only one county? 
# Ask Paulina what to do about these multi-county PWSs 

TX_FIPS[which(TX_FIPS$COUNTY %in% c("Dallas", "Johnson")),]
# this is one ranch, likely in Dallas OR johnson county, not both
# the street address is johnson county and since this is the PWS for one ranch, I'm going to say it only resides in one county. 

plot(sdwis_fips$LON, sdwis_fips$LAT) # this makes sense, about 1 pws per most counties/areas 

sdwis_fips$FIPS <- as.numeric(sdwis_fips$FIPS)
# removing the FIPS name "48NA" from the FIPS column 
data$FIPS[which(data$FIPS =="48NA")] <- NA


write.csv(sdwis_fips, "sdwis_2021.csv")
write.csv(data, "~/Downloads/joined_tx_weather.csv")


```

```{r}
county_count_sdwis <- t(data.frame(rbind(table(sdwis_fips$FIPS))))
county_count_data <- t(data.frame(rbind(table(data[c("FIPS","YEAR", "MONTH_NAME","EVENT_TYPE","DAMAGE_PROPERTY_NUM")]))))
rownames(county_count_data) == rownames(county_count_data)


df <- as.data.frame(cbind(county_count_sdwis, county_count_data, rownames(county_count_data)))
colnames(df) <- c("sdwis", "weather", "FIPS")
df$FIPS <- sub("X", "", df$FIPS)

df$sdwis <- as.numeric(df$sdwis)
df$weather <- as.numeric(df$weather)

# Seeing if there is any overlap in top counts
head(sort(county_count_sdwis, decreasing = TRUE), 20) %in% head(sort(county_count_data, decreasing = TRUE), 20) 

plot(df$weather, df$sdwis)

# looking at correlations
out <- lm(weather ~ sdwis, data = df)
summary(out)
plot(out)

out <- lm(sdwis ~ weather, data = df)
summary(out)
plot(out)

cor(df$sdwis,df$weather) # overall kind of low - maybe split by region? 

head(df)
df_w_merge <- merge(df, TX_FIPS, by.x="FIPS", by.y="FIPS", all.x=TRUE)

table(df_w_merge$FE_AREA)
summary(df_w_merge$FE_AREA)
ee <- df[which(df_w_merge$FE_AREA == "ee"),]
cor(as.numeric(ee$sdwis),as.numeric(ee$weather))


```


```{r}
tceq_df <- read_csv("~/Downloads/PIR54466_Health_Based_Violations.csv")
tceq_df$BEGIN_DT <- as.Date(tceq_df$BEGIN_DT, format ="%m/%d/%Y")
tceq_df$END_DT <- as.Date(tceq_df$END_DT, format ="%m/%d/%Y")
hist(tceq_df$VIOLATION_YEAR)
unique(tceq_df$VIOLATION_YEAR)

sdwis_tceq <- merge(tceq_df, sdwis_fips, by.x="WSID", by.y="PWS.ID", all.x=TRUE)

write.csv(sdwis_tceq, "~/Downloads/merged_tceq_sdwis_2015_2020.csv")

hist(sdwis_tceq$VIOLATION_YEAR)
#plot(sdwis_tceq$X..of.Violations,sdwis_tceq$BEGIN_DT) # are the same violations being repeated?? 

sort(table(sdwis_tceq$"ANALYTE/RULE")) # the largest group of violations is trihalomethanes -- something I did not see mentioned in any of the literature about Texas water quality but was the major thing in the Flint water crisis 
length(unique(sdwis_tceq$FIPS)) # only about 85% of counties in texas have violations no biggie 

out <- lm(X..of.Violations ~ X..of.Facilities + X..of.Site.Visits + Population.Served.Count + LAT + LON + FIPS + ZONE + BEGIN_DT_VIOL + END_DT + VIOL_NUM + POPULATION + VIOLATION_YEAR, data = sdwis_tceq) 
summary(out)

plot(sdwis_tceq$X..of.Violations, sdwis_tceq$VIOL_NUM) # looks like VIOL_NUM is actually a factor with the number of the violation and not a continuous variable, might be interesting to do some sort of categorical thing
plot(sdwis_tceq$ZONE, sdwis_tceq$X..of.Violations) # not sure what the zone variable is but it looks like one value around 40 has a very high amount of violations and maybe we are missing data for 360-410ish 
plot(sdwis_tceq$FIPS, sdwis_tceq$X..of.Violations) # again it looks like one county has a large amount of violations, otherwise it is a somewhat uniform distribution 
plot(sdwis_tceq$LON, sdwis_tceq$X..of.Violations) # this does not seem to really show anything other than what the other graphs have shown - coastlines seem susceptible to water damage 
plot(sdwis_tceq$X..of.Site.Visits, sdwis_tceq$X..of.Violations) # kind of an interesting plot, showing more of a spread out depiction of violations, not the exact same as the other graphs with mostly uniform and one spike. This one has a few scattered points where high violations =/= high site visits. 
plot(sdwis_tceq$X..of.Facilities, sdwis_tceq$X..of.Violations) # doesn't seem to be all that correlated 

```

```{r}
library(waffle)
library(ggplot2)
library(see)

violations <- sort(table(sdwis_tceq$"ANALYTE/RULE"), decreasing = TRUE)[1:12]
color <- metro_colors()[1:12]

waffle(violations/100, rows=9,
       colors = color,
       title="PWS Violation Types (N >100)", 
       xlab="1 square = 100 violations (Total: 12687)")


events <- sort(table(data$"EVENT_TYPE"), decreasing = TRUE)
color <- metro_colors()[c(2,5,8)]

waffle(events/50, rows=9,
       colors = color,
       title="Weather Event Types", 
       xlab="1 square = 50 events (Total: 6538)")

```

# Important Weather Events Pre 2015? 
```{r}
summary(data$DAMAGE_PROPERTY_NUM) # 3rd quartile is $4000 plus 

# Defined by Top Quartile of Property Damage Amount 
top_quart <- data[which(data$DAMAGE_PROPERTY_NUM >= 4000),]
table(top_quart$YEAR) # 2316 events from 2001-2014 of note, 1428 in/after 2015

# Defined by Over $1 Mil of Property Damage Amount 
top_mil <- data[which(data$DAMAGE_PROPERTY_NUM >= 1000000),]
table(top_mil$YEAR) # 242 events from 2001-2014, 139 in/after 2015 

# Defined by Over $100,000 of Property Damage Amount 
top_thou <- data[which(data$DAMAGE_PROPERTY_NUM >= 100000),]
table(top_thou$YEAR) # 674 events from 2001-2014, 451 in/after 2015 


plot(top_mil$BEGIN_LON, top_mil$BEGIN_LAT)
```
# Regression with a Binary dependent variable when the storm hits Pre and post violations 

idea: create a binary variable for each violation - does this violation happen before or after x major event? 
Regress against this variable 
Major events being $1,000,000+ in property damage 
most counties have more than one PWS and ~1 major event in the last 5 years 

```{r}
library(dplyr)
data_recent <- top_mil[top_mil$YEAR >= 2015,] # filtering to 2015+ 
data_recent <- data_recent %>% select(!starts_with("TOR") & !starts_with("CZ") & !contains("MAGNITUDE") & !contains("STATE")) # deleting some unnecessary columns
                                      
table(data_recent$FIPS)
head(data_recent)

table(sdwis_tceq$FIPS)

#ols <- glm(,family = binomial(link="logit"), data=data)

```

# Not sure how to do top analysis 
- starting with top damage in each FIPS code 
Model 1 - Reviewed 7/23 with Paulina 
** FIXED July 25 to 2015+ data 

```{r}
library(lubridate)
library(tidyverse)
library(broom)
data = data[data$YEAR >=2015,]
max_per_fips <- data %>% group_by(FIPS) %>% slice(which.max(DAMAGE_PROPERTY_NUM))
max_per_fips$BEGIN_DATE_TIME <- as.Date(dmy_hms(max_per_fips$BEGIN_DATE_TIME))

class(sdwis_tceq$BEGIN_DT)
# dummy variable for whether a violation was before or after an event in a FIPS code 
#sdwis_tceq$BEFORE_EVENT <- which(sdwis_tceq$FIPS == max_per_fips$FIPS)

sdwis_tceq <- rename(sdwis_tceq, BEGIN_DT_VIOL =BEGIN_DT_VIOL) 
max_per_fips <- rename(max_per_fips, BEGIN_DT_EVENT = BEGIN_DATE_TIME) 
merged_df <- merge(sdwis_tceq,max_per_fips, by="FIPS", all.x=TRUE) 

#ifelse(sdwis_tceq$BEGIN_DT >= max_per_fips$BEGIN_DATE_TIME, 1, 0)

#if violation began after event, put 1 in dummy column 
merged_df$dummy <- ifelse(merged_df$BEGIN_DT_VIOL >= merged_df$BEGIN_DT_EVENT, 1, 0)

ols <- glm(dummy~DAMAGE_PROPERTY_NUM + VIOLATION_YEAR + POPULATION + X..of.Facilities + X..of.Site.Visits,family = binomial, data=merged_df)
summary(ols)
plot(ols)
probabilities <- predict(ols, type = "response")
hist(probabilities)
predicted.classes <- ifelse(probabilities > 0.5, "after event", "before event")
table(predicted.classes)
head(predicted.classes)

#plot(merged_df$DAMAGE_PROPERTY_NUM, merged_df$dummy)
#cov(merged_df$Population.Served.Count,merged_df$POPULATION)

# trying with randomly generated data out of curiosity 
#merged_df$dummy_fake <- sample(c(0,1), replace = TRUE,size=length(merged_df$dummy))

# fake data, nothing is significant :) 
#ols_f <- glm(dummy_fake~DAMAGE_PROPERTY_NUM + X..of.Facilities + X..of.Violations  +  X..of.Site.Visits + Population.Served.Count + POPULATION,family = binomial(link="logit"), data=merged_df)
#summary(ols_f)


```

# Suggested Model 1 
glm logistic regression model 
finding probability that you have a violation 
aggregate violations per month on a county level 
0 if no 1 if yes violation given that there is an event in that month 
NOTE: make sure there are months with no violations 

```{r}
data_1 <- read.csv("~/Downloads/joined_tx_weather.csv")
sdwis_tceq_1 <- read.csv("~/Downloads/merged_tceq_sdwis_2015_2020.csv")
data_1 = data_1[data_1$YEAR >=2015,]
top_mil <- data_1[which(data_1$DAMAGE_PROPERTY_NUM >= 100000),]
top_mil$BEGIN_DATE_TIME <- as.Date(dmy_hms(top_mil$BEGIN_DATE_TIME))

#sdwis_tceq <- rename(sdwis_tceq, BEGIN_DT_VIOL = BEGIN_DT) 
top_mil <- rename(top_mil, BEGIN_DT_EVENT = BEGIN_DATE_TIME) 

#decided not to merge 
#colnames(county_count_events) <- paste("EV", colnames(county_count_events), sep = "_")
#colnames(county_count_sdwis) <- paste("SDW", colnames(county_count_sdwis), sep = "_")
#merged_df_2 <- merge(sdwis_tceq,top_mil, by="FIPS", all.x=TRUE) 

#making month and year format columns 
top_mil$Month_Yr_EVENT <- format(as.Date(top_mil$BEGIN_DT_EVENT), "%Y-%m")
sdwis_tceq$Month_Yr_VIOL <- format(as.Date(sdwis_tceq$BEGIN_DT_VIOL), "%Y-%m")

# making matrices with counts by county and month/year
county_count_events <- t(data.frame(rbind(table(top_mil[c("FIPS","Month_Yr_EVENT")]))))
county_count_sdwis <- t(data.frame(rbind(table(sdwis_tceq_1[c("FIPS","Month_Yr_VIOL")]))))
true_events <- county_count_events >0 # matrix with TRUE when event happens 
# table of weather events dates 
table <- t(t(apply(true_events, 1, function(u) paste( names(which(u)), collapse=","))))
count_events_df <- true_events %>% as.table %>% as.data.frame(., stringsAsFactors=FALSE)
#count_events_df <- count_events_df[count_events_df$Freq == TRUE,]

#formatting same as other dataset
count_events_df$Var1 <- sub("X", "", count_events_df$Var1)
count_events_df$Var1 <- sub(".", "-", count_events_df$Var1, fixed = TRUE)

viol_counts <- as.data.frame(table(sdwis_tceq_1[c("FIPS","Month_Yr_VIOL")]))

count_df <- merge(count_events_df, viol_counts, by.x=c("Var1", "Var2"), by.y=c("Month_Yr_VIOL","FIPS"),all=TRUE)
count_df$Freq.x[is.na(count_df$Freq.x)] <- 0 # making missing values for No event equal to 0 
#count_df <- count_df[na.omit(count_df$Freq.x),]# making missing values for No event equal to 0 
#final <- na.omit(count_df)

ols <- glm(Freq.x ~ Freq.y, data=count_df, family = binomial)
summary(ols)

x_freq <- seq(0,50,1) # prediction list 
probabilities <- predict(ols, list(Freq.y = x_freq), type = "response")
summary(probabilities)
plot(count_df$Freq.y, count_df$Freq.x, pch = 16, xlab = "EVENTS (Prop Damage > 100K)", ylab = "Freq.y")
lines(x_freq, probabilities)

```
```{r}
library(readr)
data_1 <- read_csv("joined_tx_weather.csv")
sdwis_tceq_1 <- read_csv("merged_tceq_sdwis_2015_2020.csv")

top_mil = data_1 %>% filter(YEAR >=2015& DAMAGE_PROPERTY_NUM >= 100000) %>% 
  mutate(BEGIN_DATE_TIME= as.Date(dmy_hms(BEGIN_DATE_TIME)), Month_Yr_EVENT=format(as.Date(top_mil$BEGIN_DT_EVENT), "%Y-%m"),) %>% 
  rename(BEGIN_DT_EVENT = BEGIN_DATE_TIME)

sdwis_tceq_1=sdwis_tceq_1 %>% mutate(Month_Yr_VIOL=format(as.Date(BEGIN_DT), "%Y-%m"))

county_count_events=top_mil %>% group_by(FIPS,Month_Yr_EVENT) %>% count() %>% filter(n>0)
county_count_sdwis=sdwis_tceq_1 %>% group_by(FIPS,Month_Yr_VIOL) %>% count()

count_df <- merge(county_count_events, county_count_sdwis, by.x=c("FIPS","Month_Yr_EVENT"),
                  by.y=c("FIPS","Month_Yr_VIOL"),all=TRUE) %>% rename(Events=n.x, Violations=n.y) %>% replace(is.na(.), 0) %>% 
  mutate(Vio.dum=ifelse(Violations>0,1,0)) #Maybe change for >mean(Violations or another threshold)

ols <- glm(Vio.dum ~ Events, data=count_df, family = binomial)
summary(ols)

plot(count_df$Events, count_df$Violations, ylab="Violations", xlab="Events", main="All types of violations")

```
## Seems to have almost the exact same results for when filtered to have only TTHM violations 
```{r}
county_count_tthm= sdwis_tceq_1 %>% rename(ANALYTE.RULE="ANALYTE/RULE") %>% dplyr::filter(ANALYTE.RULE == "TTHM") %>% group_by(FIPS,Month_Yr_VIOL)  %>% count() 

count_df <- merge(county_count_events, county_count_tthm, by.x=c("FIPS","Month_Yr_EVENT"),
                  by.y=c("FIPS","Month_Yr_VIOL"),all=TRUE) %>% rename(Events=n.x, Violations=n.y) %>% replace(is.na(.), 0) %>% 
  mutate(Vio.dum=ifelse(Violations>0,1,0)) #Maybe change for >mean(Violations or another threshold)

ols <- glm(Vio.dum ~ Events, data=count_df, family = binomial)
summary(ols)

plot(count_df$Events, count_df$Violations, ylab="Violations", xlab="Events", main="TTHM violations")

```

```{r}
# SURFACE WATER violations 
county_count_sw= sdwis_tceq_1 %>% dplyr::filter(SOURCE_TYPE=="SW"|SOURCE_TYPE=="SWP") %>% group_by(FIPS,Month_Yr_VIOL)  %>% count() 

count_df <- merge(county_count_events, county_count_sw, by.x=c("FIPS","Month_Yr_EVENT"),
                  by.y=c("FIPS","Month_Yr_VIOL"),all=TRUE) %>% rename(Events=n.x, Violations=n.y) %>% replace(is.na(.), 0) %>% 
  mutate(Vio.dum=ifelse(Violations>0,1,0)) #Maybe change for >mean(Violations or another threshold)

ols <- glm(Vio.dum ~ Events, data=count_df, family = binomial)
summary(ols)

plot(count_df$Events, count_df$Violations, ylab="Violations", xlab="Events", main="Surface Water violations")

# GROUND WATER violations 
county_count_gw= sdwis_tceq_1 %>% dplyr::filter(SOURCE_TYPE=="GW"|SOURCE_TYPE=="GWP") %>% group_by(FIPS,Month_Yr_VIOL)  %>% count() 

count_df <- merge(county_count_events, county_count_gw, by.x=c("FIPS","Month_Yr_EVENT"),
                  by.y=c("FIPS","Month_Yr_VIOL"),all=TRUE) %>% rename(Events=n.x, Violations=n.y) %>% replace(is.na(.), 0) %>% 
  mutate(Vio.dum=ifelse(Violations>0,1,0)) #Maybe change for >mean(Violations or another threshold)

ols <- glm(Vio.dum ~ Events, data=count_df, family = binomial)
summary(ols)

plot(count_df$Events, count_df$Violations, ylab="Violations", xlab="Events", main="Ground Water violations")
```



```{r}

##Exploring the violations a little bit
sdwis_tceq_1 %>% group_by(SOURCE_TYPE) %>% count() %>% ggplot(aes(x=SOURCE_TYPE, y=n, fill=SOURCE_TYPE))+geom_bar(stat="identity")+
  labs(y="SDWIS MCL Violations", title="Texas")+theme_classic()

#Increase in TTHM and coliforms could be a result of floods after a while (more orgnanic matter that gets converted to TTHMs in desinfection). Nitrate perhaps too
sdwis_tceq_1 %>% filter(SOURCE_TYPE=="GW"|SOURCE_TYPE=="GWP")%>%group_by(`ANALYTE/RULE`) %>% count() %>%  filter(n>50)%>% 
  ggplot(aes(x=`ANALYTE/RULE`, y=n, fill=`ANALYTE/RULE`))+geom_bar(stat="identity")+
  labs(y="SDWIS MCL Violations", title="Utilites sourced with GW")+theme_classic()+ theme(axis.ticks.x = element_blank(),
                                                                                       axis.text.x = element_blank())

##TTHM, coliforms, and TUrbidity can increase after a flood (runoff + more organic material that produces TTHMs with Chlorine)
#GW = ground water, SW = surface water
sdwis_tceq_1 %>% filter(SOURCE_TYPE=="SW"|SOURCE_TYPE=="SWP")%>%group_by(`ANALYTE/RULE`) %>% count() %>%  filter(n>50)%>% 
  ggplot(aes(x=`ANALYTE/RULE`, y=n, fill=`ANALYTE/RULE`))+geom_bar(stat="identity")+
  labs(y="SDWIS MCL Violations", title="Utilites sourced with SW")+theme_classic()+ theme(axis.ticks.x = element_blank(),
                                                                                          axis.text.x = element_blank())


```

code that don't make sense 
```{r}

# compare values of 1 row in count_events_df to find value for amt of violations / whether violations exist in that month 

# initializing empty DF 
dfn <- data.frame(Date=character(), # each month/year 
              FIPS=character(), # county
              VIOL_GIVEN_EVENT=numeric(), # 0,1,or NA
              EVENT = numeric(), # 0 or 1 
              stringsAsFactors=FALSE) 

# running loops to try to fill out the DF with date, fips, violation given event, and whether an event happened 
# k is row*col to have space for every combination of date and fips 
# j is FIPS 
# i is Date 
"
dim_length <- length(colnames(county_count_sdwis))*length(rownames(county_count_sdwis))
for(k in 1:dim_length){
  for(j in 1:length(colnames(county_count_sdwis))){
    for(i in 1:length(rownames(county_count_sdwis))){
      if(rownames(county_count_sdwis)[i] %in% count_events_df$Var1 & 
        colnames(county_count_sdwis)[j]%in% count_events_df$Var2 & 
        county_count_sdwis[i,j] >0) {
        dfn[j,] <- c(rownames(county_count_sdwis)[j], colnames(county_count_sdwis)[i], 1,1)
      }
      if(rownames(county_count_sdwis)[i] %in% count_events_df$Var1 & 
        colnames(county_count_sdwis)[j]%in% count_events_df$Var2 &
        county_count_sdwis[i,j] == 0){
        dfn[j,] <- c(rownames(county_count_sdwis)[j], colnames(county_count_sdwis)[i], 0,1)
      }
      if(rownames(county_count_sdwis)[i] %!in% count_events_df$Var1 | 
        colnames(county_count_sdwis)[j] %in% count_events_df$Var2){
        dfn[j,] <- c(rownames(county_count_sdwis)[j], colnames(county_count_sdwis)[i], NA,0)
      }
    }
  }
}"
```


# Suggested Model 2 - ON HOLD 
glm logistic regression model 
categorical Variable of county (FIPS)
three variables - 
1) presence/not of an event in a month 
2) county variable 
3) event type 
```{r}

```


# TF-IDF 




